{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "os.chdir(\"c:\\\\Users\\\\erica\\\\OneDrive - ETH Zurich\\\\Exchange\\\\Machine Learning\\\\CS349---Group8\\\\HW4\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tests import test_bandit\n",
    "from tests import test_q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erica\\OneDrive - ETH Zurich\\Exchange\\Machine Learning\\CS349---Group8\\HW4\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:197: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\erica\\OneDrive - ETH Zurich\\Exchange\\Machine Learning\\CS349---Group8\\HW4\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:210: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\erica\\OneDrive - ETH Zurich\\Exchange\\Machine Learning\\CS349---Group8\\HW4\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\erica\\OneDrive - ETH Zurich\\Exchange\\Machine Learning\\CS349---Group8\\HW4\\.venv\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "test_bandit.test_bandit_simple()\n",
    "test_bandit.test_bandit_slots()\n",
    "test_bandit.test_bandit_random_argmax()\n",
    "test_bandit.test_bandit_frozen_lake()\n",
    "test_bandit.test_bandit_deterministic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available actions: 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QLearning' object has no attribute 'Q'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_q_learning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_q_learning_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_q_learning\u001b[38;5;241m.\u001b[39mtest_q_learning_slots()\n\u001b[0;32m      3\u001b[0m test_q_learning\u001b[38;5;241m.\u001b[39mtest_q_learning_frozen_lake()\n",
      "File \u001b[1;32mc:\\Users\\erica\\OneDrive - ETH Zurich\\Exchange\\Machine Learning\\CS349---Group8\\HW4\\tests\\test_q_learning.py:18\u001b[0m, in \u001b[0;36mtest_q_learning_simple\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m env \u001b[38;5;241m=\u001b[39m gymnasium\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimpleEnv-v0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearning()\n\u001b[1;32m---> 18\u001b[0m _, rewards \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rewards) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould have one reward per step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(rewards \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach bin contains its own reward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\erica\\OneDrive - ETH Zurich\\Exchange\\Machine Learning\\CS349---Group8\\HW4\\src\\q_learning.py:108\u001b[0m, in \u001b[0;36mQLearning.fit\u001b[1;34m(self, env, steps, num_bins)\u001b[0m\n\u001b[0;32m    106\u001b[0m     action \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(available_actions)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m     optimal_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ\u001b[49m \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    109\u001b[0m     action \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(optimal_actions)\n\u001b[0;32m    111\u001b[0m state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'QLearning' object has no attribute 'Q'"
     ]
    }
   ],
   "source": [
    "test_q_learning.test_q_learning_simple()\n",
    "test_q_learning.test_q_learning_slots()\n",
    "test_q_learning.test_q_learning_frozen_lake()\n",
    "test_q_learning.test_q_learning_random_argmax()\n",
    "test_q_learning.test_q_learning_deterministic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
