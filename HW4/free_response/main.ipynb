{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "os.chdir(\"c:\\\\Users\\\\erica\\\\OneDrive - ETH Zurich\\\\Exchange\\\\Machine Learning\\\\CS349---Group8\\\\HW4\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tests import test_bandit\n",
    "from tests import test_q_learning\n",
    "import q2\n",
    "import q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\USERS\\ERICA\\ONEDRIVE - ETH ZURICH\\EXCHANGE\\MACHINE LEARNING\\CS349---GROUP8\\HW4\\.VENV\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:197: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\USERS\\ERICA\\ONEDRIVE - ETH ZURICH\\EXCHANGE\\MACHINE LEARNING\\CS349---GROUP8\\HW4\\.VENV\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:210: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\USERS\\ERICA\\ONEDRIVE - ETH ZURICH\\EXCHANGE\\MACHINE LEARNING\\CS349---GROUP8\\HW4\\.VENV\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\USERS\\ERICA\\ONEDRIVE - ETH ZURICH\\EXCHANGE\\MACHINE LEARNING\\CS349---GROUP8\\HW4\\.VENV\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "test_bandit.test_bandit_simple()\n",
    "test_bandit.test_bandit_slots()\n",
    "test_bandit.test_bandit_random_argmax()\n",
    "test_bandit.test_bandit_frozen_lake()\n",
    "test_bandit.test_bandit_deterministic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q_learning.test_q_learning_simple()\n",
    "test_q_learning.test_q_learning_slots()\n",
    "test_q_learning.test_q_learning_frozen_lake()\n",
    "test_q_learning.test_q_learning_random_argmax()\n",
    "test_q_learning.test_q_learning_deterministic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bandits versus Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running..\n",
      "Done! See plots in `free_response/2a*.png`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q2.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.\n",
    "For FrozenLake Comparison, the QLearning model quckily achieves higher reward - 0.12 in average while Bandits model's reward remains at 0.\n",
    "\n",
    "For SlipperyFrozenLake Comparison, QLearning and Bandits model both produces low and fluctuating rewards.\n",
    "\n",
    "For SlotMachines Comparison, both QLearning and Bandits model have volatile and high rewards. However, Bandits model's reward is relatively more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "FrozenLake Comparison appears to have a higher reward on average for QLearning than for MultiArmedBandit, this is because QLearning learns an optimal quality value by taking account all possible future states. Overtime, it uses these quality value estimates to consistently select actions that lead to higher long-term rewards. While MultiArmedBandit only considers the action locally which is not able to identify the optimal strategy in a state dynamicly changing environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "There is no tunning of hyperparameters that makes MultiArmedBandit perform as well as the QLearning. This is because MultiArmedBandit considers each action as independent from each other. This would cause the MultiArmedBandit unable to use state transition and future rewards to identify the optimal long term strategy. Therefore, no matter how fine-tuned the hyperparameters of the MultiArmedBandit are, MultiArmedBandit can not perform as well as QLearning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\n",
    "Slot Machine Comparison appears to have a higher reward on average for MultiArmedBandit than for QLearning. This is because for slot machines, all the actions are independent to each other. This property allows MultiArmedBandit to quickly learns the arms that produce highest reward. QLearning's algorithm would complecate the learning process and may cause the learning reward to be lower and more fluctuated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.\n",
    "We are able to tune the hyperparameters to make QLearning to perform as well as the MultiArmedBandit, by adjusting the hyperparameter gamma close to 0 would allow the MultiArmedBandit consider less of the future state. And by lowering the alpha hyperparameter, we are able to value more of the current experience. In the end, adjusting these hyperparameters allows the QLearning to mimic the MultiArmedBandit learning process which allows QLearning to perform as well as the MultiArmedBandit in particular situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration versus Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to free_response/3a_g0.9_a0.2.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q3.q3a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.\n",
    "The code runs QLearing in epsilons from low to high, and compares the performance of QLearing for different exploration and exploition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\n",
    "For epsilon = 0.8, the reward value remains near zero, epsilon = 0.008 it has learned some rewards over the steps but is as high as when epsilon = 0.08. Therefore, epsilon = 0.08 is the 'best' value. When epsilon is too large (0.8), it would explore to much and did not expoit enough to obtain consistant better rewards; when epsilon is too small (0.008) the model expoit too much on the current best actions and may not explore sufficiently to identify the optimal action to take. Therefore, the middle value - 0.08 achieves the balance of getting consistant and good actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\n",
    "For epsilon = 0.8, it would remain low reward when the timesteps increased as its low expoition rate prevents it from achieving consistant optimal strategy.\n",
    "\n",
    "For epsilon = 0.08, it would achieve a better results in the beginning but would not improve in a long term as the expoitation is not enough.\n",
    "\n",
    "For epsilon = 0.008, it would have the best performance in the long term as with enough attempts, the low exploration rate would explore enough actions and low epsilon's high expoitation rate allows it to find optimal strategy at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.\n",
    "Tuning the epsilons with too many choices may result in overfitting of the model. Therefore, if we want to use the agent in a new domain, it may not be perform as well as on the old domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
