Question 2:
In implementing the k-nearest neighbors (k-NN) classifier, we utilized both Euclidean distance and Cosine Similarity as metrics to address various types of feature spaces. To improve computational efficiency, we applied dimensionality reduction techniques to reduce data size while retaining key feature patterns. We also experimented with feature scaling to enhance Euclidean-based distance performance and transformed certain attributes to binary for tasks with binary class distinctions.
For hyper-parameters, we set k=3 for k-NN to balance accuracy and efficiency and selected 10 clusters in the k-means clustering phase to accommodate class diversity. Initial cluster means for k-means were randomly sampled from the training data to introduce variability, with distances recalculated using cosine similarity.
On the validation set, the classifier achieved an accuracy of 0.85, precision of 0.86, recall of 0.85, and an F1-score of 0.85. For the test set, results improved with an accuracy of 0.895, precision of 0.89, recall of 0.89, and an F1-score of 0.88. The 10x10 confusion matrix indicates strong performance across most classes, though certain classes (e.g., class 5 with confusion between labels 5 and 3) exhibited some misclassification, which could be addressed by further tuning or feature engineering.