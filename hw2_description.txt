Question 2:
To implement the k-nearest neighbors (k-NN) classifier, we used both Euclidean distance and Cosine Similarity to measure similarity, allowing the model to handle different types of feature spaces. To address the computational load of k-NN, we employed dimensionality reduction by transforming features into a lower-dimensional space, reducing data size while preserving key patterns. We experimented with feature scaling to optimize Euclidean-based distances, and mapped certain attributes to binary to enhance performance in binary classification tasks.
For hyper-parameter choices, we set k=3 for k-NN, balancing accuracy and computational efficiency, and selected 10 clusters for k-means based on expected class diversity. In the training phase, initial cluster means for k-means were randomly chosen from the training data, which provided variability for initial clustering while using cosine distance for reassigning points to clusters.
After finalizing the model on the validation set, we ran it on the test set, generating a 10x10 confusion matrix for each distance metric. In our analysis, we found that Cosine Similarity yielded better accuracy in cases where feature vectors were directional, while Euclidean distance was effective for cases with clustered feature magnitudes.